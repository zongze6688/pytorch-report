# pytorch 基本实验报告

## 前言
本文是一个简短的pytorch入门实验报告。

本报告中提及的所有代码均在：

由于此pytorch入门实验几乎完全在碎片化时间中完成，因此**大多数相关代码都被AI润色**或**大篇幅修改**过，但所有核心代码最初都为手工完成。

本文所有观点/结论均为**个人观点**，部分甚至**没有严谨的实验数据支撑**

本文完全手工完成，**没有任何AI生成的部分**，可以放心阅读 ！！！

## Task 1 ： 使用深度学习文本分类

使用多种深度学习方法，对new_train.csv / new_test.csv 进行文本的情感分类。文本的标签范围是0、1、2、3、4，分别对应**极其负面**（0）至**非常正面**（4）

### 1. **手动实现单个线性层**

通过自定义softmax, crossentropy-loss, SGD, mean-squared-loss, 实现了单个线性层的训练，详见**pytorch_task1.py**。
这个方法中没有使用torch.nn中的任何函数

### 2. **带有残差连接的线性层**

手动实现过程没有使用正则化技术，因此效果不佳（acc 26%左右.....）

在**pytorch_task1_MLP.py**中，实现了带有残差网络的全连接层，并使用dropout正则化和Adam优化器进行训练。准确率接近50%。

### 3. **卷积网络（CNN）**

在**pytorch_task1_CNN.py**中，加入了**词嵌入（word_embedding）**,并探索了多种卷积网络，包括：

1. 较浅的卷积网络及其优化版本
2. 较深的卷积网络
3. 加入残差连接的深度网络
4. 多尺度深度文本CNN

实验结果表明，加深卷积网络或换用不同的卷积核并不能显著的提升模型表现。并且**多尺度卷积**和**使用与文本一样长的卷积核**（AI建议）的结果没有显著区别（acc 46%左右）

### 4. **循环神经网络**

在**pytorch_task1_RNN.py**中，探索了使用循环网络进行文本分类，尝试了**RNN GRU LSTM**并使用num_layers=2的双向网络，发现最终效果不是很理想（acc 45%左右）。在引入注意力机制后，效果有所提升（acc 接近50%）

### 5. **Transformer**

在**pytorch_task1_Transfromer.py**中，使用了传统的transformer进行训练（**位置编码+多头自注意力**）。然而，效果并不理想（acc 47%左右）

### 6. **微调BERT模型**
 
鉴于以上所有模型的效果均不理想，我分别使用**BERT-base**和**BERT-large**在数据集上进行微调（见**BERT_for_pytorch_tesk.py**）。微调后，BERT-large准确率大约68%，BERT-base准确率大约63%。尽管模型准确率看起来并不高，但模型在对抗样本上表现良好，这说明BERT模型已经能够较好的理解句子的情感色彩。

运行**reuse_BERT.py**可以让模型进行在线预测

### Task 1 总结

鉴于不同模型在训练数据上的表现均不是很高，我认为这个数据可能就是训练不太好，但从BERT模型表现来看，这个数据仍然非常有意义。

总结上面的不同模型，排除微调BERT以外，准确率最好的模型是**带有残差连接的线性层**和**加入注意力机制的LSTM**（均为50%左右）。Transformer模型的表现并不是很好，可能是因为数据量过少，而transformer通常需要大量训练数据。

## Task 2 ： 使用Transformer实现3+3位数加法（有待深入研究...）

### **实验内容**

使用encoder-decoder的完整Transformer进行3+3位数加法的训练，所有数字均转化成字符，保留‘+’号。**每个epoch都随机初始化训练样本一次。**实验代码在**Transformer_3_3.py**。

在训练50个epoch之后，模型开始能够进行一些模糊的加法，并且对数字的大小有一定感知。然而，在绝大多数情况下，模型仍然无法做出准确的加法。

**不足之处** ： 由于训练数据理论上来说是有限的（3+3位数），在每轮训练时又会随机初始化，因此模型会倾向于“记住”所有的内容，导致泛化能力很差。因此，可能需要**更正模型的结构**，**改变某些超参数**，以改进模型的性能

## Task 3 ： 构建一个自己的**诗歌写作**语言模型

### **实验内容**

使用chinese-poetry数据集进行训练，包括训练集388599条，测试集1710条。（本试验中没有使用测试集）

由于诗歌生成通常不需要过多的prompt，因此使用decoder-only的模型，训练20个epoch。（经过尝试，encoder-decoder的模型难以训练且效果很差）

### **模型结构总结**

模型的训练和构造中使用了如下方法：

1. 因果掩码 (Causal Mask)，防止模型关注未来位置
2. 层规范化 (Layer Norm) 相比于batch norm更加稳定
3. Sin/Cos 位置编码 
4. GELU 激活函数，在FFN中替代传统ReLU，提供更平滑的梯度
5. 更大的FFN维度：ffn_dim=1024 > embed_dim=768，或许可以提升模型能力（**胡乱猜测......**）
6. 梯度裁剪  grad_clip=1.0 防止梯度爆炸
7. 权重衰减 weight_decay=0.01 
8. 余弦退火学习率调度 使学习率平滑衰减
9. 较高的dropout=0.3 **训练数据量不大，防止过拟合**
10. 保留检查点：定期保存中间检查点支持断点恢复训练（**吸取之前的教训......**）

直接运行**generate_poem.py**即可生成诗歌。

### **生成示例**

- 示例 1 (提示: '无'):
  - **天下长云不可留 归鸦依旧晚风收 何须更上南山宿 曾伴落叶舟**

- 示例 2 (提示: '难忘旧情'):
  - **难忘旧情  我亦无人知**

- 示例 3 (提示: '青楼一夜'):
  - **青楼一夜 寒霞 春雨阴阴漾碧霞 却恨不知春色近 一枝先识旧梅花**

- 示例 4 (提示: '春风得意'):
  - **春风得意  岁月亦高楼 竹草无新绿 山云有旧愁 水云清影入 草树晚阴浮 忽忽无知己 閒怀亦自由**

- 示例 5 (提示: '借酒浇愁'):
  - **借酒浇愁 未醒 不如一夜为谁醒 无端独向青灯里 不是人间说旧灯**

### 不足之处

虽然模型已经能够生成复合基本要求的诗歌，但仍然有很多不足之处，包括但不限于 :

- **诗句不符合逻辑**
- **没有平仄押韵** 
- **没有对仗**（但是这个数据集是诗+词）
